import threading
import sys
import os
import random
import asyncio
import queue
import librosa
import sounddevice as sd

from PIL import Image
from moviepy.editor import AudioFileClip, ImageClip, VideoFileClip, CompositeVideoClip, concatenate_videoclips, ColorClip, vfx
from render import FrameData
from vts_hotkey_trigger import VTubeStudioHotkeyTrigger
from utils import save_as_temp_file

# ä¸€ã¤ä¸Šã®éšå±¤ã®ãƒ‘ã‚¹ã‚’å–å¾—
parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.append(parent_dir)

from vtuber_camera import VTuberCamera
from utils import *


# ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã®ä½œæˆ
class Timeline:
    def __init__(self, frame_data_list: list[FrameData], output_file: str, background_video_path: str):
        self.vtuber_camera = VTuberCamera()
        self.hotkey_trigger = VTubeStudioHotkeyTrigger()

        # ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¯ãƒªãƒƒãƒ—ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
        self.final_clip = None

        self.frame_clips = []
        self.subtitle_image_clips = []
        self.streaming_clips = []  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã®æ˜ åƒã‚¯ãƒªãƒƒãƒ—ã‚’ä¿æŒã™ã‚‹ãƒªã‚¹ãƒˆ

        self.background_video_path = background_video_path
        self.preview_height, self.preview_width = 1080,1920 # è§£åƒåº¦ã‚’å–å¾—

        self.hotkeys = []

        self.AUDIO_DEVICE_INDEX = 78 # 78 CABLE-A Input (VB-Audio Cable A , WASAPI (0 in, 2 out) - 48000.0 Hz
        self.device_info = sd.query_devices(self.AUDIO_DEVICE_INDEX, 'output')

        # ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã‚­ãƒ¼ã®IDã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
        self.emotion_shortcut_keys = []
        self.motion_shortcut_keys = []
        # æ—¢å­˜ã®åˆæœŸåŒ–ã‚³ãƒ¼ãƒ‰
        self.previous_emotion_shortcut = None
        self.previous_motion_shortcut = None

        self.frame_data_list = frame_data_list
        self.output_file = output_file

        self.background_video_start_time = 0# èƒŒæ™¯å‹•ç”»ã®å†ç”Ÿæ™‚é–“
        self.explanation_video_start_time = 0# è§£èª¬å‹•ç”»ã®å†ç”Ÿæ™‚é–“
        self.previous_video_duration = 0# å‰ã®å‹•ç”»ã®é•·ã•


    # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã®ä½œæˆ
    async def create(self):

        # VTSã€€APIã€€æ¥ç¶š
        await self.hotkey_trigger.connect()
        # ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã‚­ãƒ¼ã®å–å¾—
        self.hotkeys = await self.hotkey_trigger.get_hotkeys()
        # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
        for frame_data in self.frame_data_list:
            await self._add_media_to_timeline(frame_data)
        # VTSã€€APIã€€åˆ‡æ–­
        await self.hotkey_trigger.disconnect()


        # frame_clips = [frame_data[11] for frame_data in self.frame_data_list]
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¯ãƒªãƒƒãƒ—ã‚’é€£çµã—ã¦æœ€çµ‚çš„ãªå‹•ç”»ã‚’ä½œæˆ
        frame_clip = concatenate_videoclips(self.frame_clips)
    
        # 3.subtitle_image_clipã‚’ãƒªã‚µã‚¤ã‚º
        subtitle_clip = concatenate_videoclips(self.subtitle_image_clips)
        subtitle_clip = subtitle_clip.resize(width=self.preview_width)
        subtitle_clip = subtitle_clip.set_position(("center", "bottom"))

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã®æ˜ åƒã‚’å–å¾—
        streaming_clip = concatenate_videoclips(self.streaming_clips)
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã—ãŸã‚¯ãƒªãƒƒãƒ—ã«å¯¾ã—ã¦ã‚¯ãƒ­ãƒã‚­ãƒ¼å‡¦ç†ã‚’è¡Œã†
        streaming_clip = streaming_clip.fx(vfx.mask_color, color=[0, 255, 0], thr=150, s=10)


        # streaming_video_clip_w_start = streaming_clip.w/4 -20#èª¿æ•´
        # streaming_video_clip_w_end = streaming_clip.w*3/4 +20#èª¿æ•´
        # resized_streaming_video_clip = streaming_clip.crop(x1=streaming_video_clip_w_start, x2=streaming_video_clip_w_end)
        # resized_streaming_video_clip = resized_streaming_video_clip.resize(height=self.preview_height*0.7)
        streaming_clip = streaming_clip.set_position(("center", "center"))
        # resized_streaming_video_clip = resized_streaming_video_clip.set_position(("center", "center"))

        # ãƒ›ãƒ¯ã‚¤ãƒˆãƒœãƒ¼ãƒ‰ç”»åƒã‚’åˆæˆ,å­—å¹•ç”»åƒã‚’åˆæˆ
        composed_stream = CompositeVideoClip([
            frame_clip,
            streaming_clip,
            subtitle_clip,
        ])

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã«åˆæˆã€€-> æœ€çµ‚çš„ã«å‹•ç”»ã¨ã—ã¦æ›¸ãå‡ºã™ã‚‚ã®
        # self.final_clip = concatenate_videoclips(composed_stream, method="compose")
        self.final_clip = composed_stream

        # å‹•ç”»ã®å†ç”Ÿã¨ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã‚­ãƒ¼ã®é€ä¿¡ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§é–‹å§‹
        # threading.Thread(target=self.play_video).start()

        # self.play_video()
    

    # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ç¶­æŒã—ãªãŒã‚‰ã€æŒ‡å®šã—ãŸæ¨ªå¹…ã¾ãŸã¯é«˜ã•ã«åŸºã¥ã„ã¦ãƒªã‚µã‚¤ã‚ºå¾Œã®å¯¸æ³•ã‚’è¨ˆç®—
    def resize_aspect_ratio(self, current_width, current_height, target_width, target_height):

        aspect_ratio = current_width / current_height
        
        if target_width is not None and target_height is not None:
            target_aspect_ratio = target_width / target_height
            if aspect_ratio > target_aspect_ratio:
                new_width = target_width
                new_height = int(new_width / aspect_ratio)
            else:
                new_height = target_height
                new_width = int(new_height * aspect_ratio)
        elif target_width is not None:
            new_width = target_width
            new_height = int(new_width / aspect_ratio)
        elif target_height is not None:
            new_height = target_height
            new_width = int(new_height * aspect_ratio)
        else:
            new_width = current_width
            new_height = current_height
        
        return new_width, new_height


    # ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ»ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¢ã‚¦ãƒˆã®å‡¦ç†
    def _fade_opacity(self, t, duration):
        fade_duration = 0.5  # ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ»ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¢ã‚¦ãƒˆã®æ™‚é–“ã‚’èª¿æ•´
        if t < fade_duration:
            opacity = t / fade_duration
        elif t > duration - fade_duration:
            opacity = (duration - t) / fade_duration
        else:
            opacity = 1
        # print(f"t: {t}, duration: {duration}, opacity: {opacity}")  # ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆ
        return opacity
    

     # ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ»ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¢ã‚¦ãƒˆã®æ™‚é–“ã‚’èª¿æ•´
    def _fade_clip(self, clip, duration, fade_duration=0.5):
        clip = clip.crossfadein(fade_duration).crossfadeout(fade_duration)
        return clip


    # é»’ã„æ ã‚’ã¤ã‘ã‚‹é–¢æ•°
    def _add_black_frame(self, explanation_clip):
        # é»’ã„æ ã‚’ä½œæˆ
        border_width = 5
        # border_color = (0, 0, 0)  # é»’è‰²ï¼ˆRGBï¼‰
        # ãƒ¬ã‚¤ãƒ³ãƒœãƒ¼ã‚¯ã‚«ãƒ©ãƒ¼ã®è¾æ›¸
        border_colors = {
            'black': (0, 0, 0),       # é»’
            'red': (255, 0, 0),       # èµ¤
            'orange': (255, 165, 0),  # ã‚ªãƒ¬ãƒ³ã‚¸
            'yellow': (255, 255, 0),  # é»„
            'green': (0, 128, 0),     # ç·‘
            'blue': (0, 0, 255),      # é’
            'indigo': (75, 0, 130),   # ã‚¤ãƒ³ãƒ‡ã‚£ã‚´
            'violet': (238, 130, 238) # ãƒã‚¤ã‚ªãƒ¬ãƒƒãƒˆ
        }
        selected_color = 'black'  # ä½¿ç”¨ã™ã‚‹è‰²ã‚’å¤‰æ•°ã§æŒ‡å®š
        border_clip = ColorClip(size=(explanation_clip.w + 2 * border_width, explanation_clip.h + 2 * border_width),
                                color=border_colors['violet']).set_duration(explanation_clip.duration)

        # å…ƒã®ã‚¯ãƒªãƒƒãƒ—ã¨é»’ã„æ ã‚’åˆæˆ
        final_clip = CompositeVideoClip([border_clip,
                                        explanation_clip.set_position((border_width, border_width))],
                                        size=border_clip.size)
        return final_clip
    

    def _add_streaming_video(self, audio_duration, audio_file):
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã®æ˜ åƒã‚’éŒ²ç”»ã™ã‚‹ãŸã‚ã®ã‚¯ãƒªãƒƒãƒ—ãƒªã‚¹ãƒˆ
        # streaming_clips = []

        # audio_thread = threading.Thread(target=self.vtuber_play_audio, args=(audio_file,))
        # audio_thread.start()

        data, samplerate = librosa.load(audio_file, sr=self.device_info['default_samplerate'])
        sd.play(data, samplerate=samplerate, device=self.AUDIO_DEVICE_INDEX)

        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒæµã‚Œã¦ã„ã‚‹é–“ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã®æ˜ åƒã‚’éŒ²ç”»
        for t in range(int(audio_duration * 24)):  # 24fpsã‚’æƒ³å®š
            # frame = self.vtuber_camera.capture_camera_frame()
            # frame = self.vtuber_camera.get_frame()
            # frame = next(self.vtuber_camera.capture_camera_frame())
            frame = next(self.vtuber_camera.get_frame())
            # frame = next(frame)
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # cv2.imshow("Frame", frame)
            # ã‚­ãƒ¼å…¥åŠ›å¾…ã¡
            # cv2.waitKey(1)

            # frame_RGBA = process_transparentize_green_back(frame_rgb)

            streaming_clip = ImageClip(frame_rgb).set_duration(1/24)  # 24fpsã‚’æƒ³å®š
            # streaming_clip = ImageClip(frame_RGBA).set_duration(1/24)  # 24fpsã‚’æƒ³å®š
            self.streaming_clips.append(streaming_clip)

        sd.wait()

        # audio_thread.join()

    #    # 2.streaming_video_clipã‚’ãƒªã‚µã‚¤ã‚º
    #     # å·¦å³ã‹ã‚‰1/4ãšã¤åˆ‡ã‚Šå–ã‚‹
    #     streaming_video_clip = concatenate_videoclips(self.streaming_clips)# ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã®æ˜ åƒã®ã‚¯ãƒªãƒƒãƒ—ã‚’çµåˆ
    #     # streaming_video_clip_w_start = streaming_video_clip.w/4 -20#èª¿æ•´
    #     # streaming_video_clip_w_end = streaming_video_clip.w*3/4 +20#èª¿æ•´
    #     # streaming_video_clip = streaming_video_clip.crop(x1=streaming_video_clip_w_start, x2=streaming_video_clip_w_end)
    #     # streaming_video_clip = streaming_video_clip.resize(height=self.preview_height*0.7)
    #     # streaming_video_clip = streaming_video_clip.set_position(("right", "center"))

    #     # çµæœã‚’ã‚­ãƒ¥ãƒ¼ã«å…¥ã‚Œã‚‹
    #     result_queue.put(streaming_video_clip)



    # ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ã®å†ç”Ÿ
    def vtuber_play_audio(self, audio_file):
        data, samplerate = librosa.load(audio_file, sr=self.device_info['default_samplerate'])
        sd.play(data, samplerate=samplerate, device=self.AUDIO_DEVICE_INDEX)
        sd.wait()


    # å„éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é †ç•ªã«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã«è¿½åŠ 
    # å¯¾å¿œã™ã‚‹è§£èª¬ç”¨ã®ç”»åƒã‚„å‹•ç”»ã‚’ã€éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®é–‹å§‹ã‹ã‚‰çµ‚äº†ã¾ã§è¡¨ç¤ºã™ã‚‹ã‚ˆã†ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã«è¿½åŠ   
    # ãƒãƒ¼ãƒãƒ£ãƒ«ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§è¡¨ç¤ºã—ãªãŒã‚‰ã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆã™ã‚‹ã€‚ãã—ã¦ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã‚­ãƒ¼ã‚‚ãã®æ™‚ã«å…¥åŠ›ã™ã‚‹ã‚ˆã†ã«ã™ã‚‹ç‚¹
    async def _add_media_to_timeline_old(self, frame_data: FrameData):

        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†ç”Ÿæ™‚é–“ã‚’å–å¾—ã€€-> ğŸŒŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒãƒ«ãƒ¼ãƒ—å†ç”Ÿã•ã‚Œãªã„ã‚ˆã†ã«ã™ã‚‹ã€‚å†ç”ŸãŒçµ‚ã‚ã£ãŸã‚‰çµ‚äº†ã™ã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
        audio_clip = AudioFileClip(frame_data.audio_file).set_fps(44100)  # 44100ã¯ä¸€èˆ¬çš„ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã§ã™
        audio_duration = audio_clip.duration
        # frame_data.audio_duration = audio_duration


        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã®æ˜ åƒã‚’éŒ²ç”»  
        result_queue = queue.Queue()
        streaming_thread = threading.Thread(target=self._add_streaming_video, args=(audio_duration, frame_data.audio_file))
        streaming_thread.start()
        
        # audio_thread = threading.Thread(target=self.vtuber_play_audio, args=(frame_data.audio_file,))
        # audio_thread.start()


        # ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã‚­ãƒ¼ã®å…¥åŠ›
        # self.hotkeys ã‹ã‚‰ Name ãŒemotion_shortcut_key ã¨ motion_shortcut_key ã¨ä¸€è‡´ã™ã‚‹hotkeyIDã‚’å–å¾—
        self.emotion_shortcut_keys = [hotkey['hotkeyID'] for hotkey in self.hotkeys if hotkey['name'] in frame_data.emotion_shortcut]
        self.motion_shortcut_keys = [hotkey['hotkeyID'] for hotkey in self.hotkeys if hotkey['name'] in frame_data.motion_shortcut]
        
        # ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã‚­ãƒ¼ã®å…¥åŠ›
        if self.motion_shortcut_keys:
            # print(f"motion_shortcut_keys: {self.motion_shortcut_keys}")
            selected_motion_shortcut = random.choice(self.motion_shortcut_keys)
            if selected_motion_shortcut != self.previous_motion_shortcut:
                await self.hotkey_trigger.trigger_hotkey(selected_motion_shortcut)
                self.previous_motion_shortcut = selected_motion_shortcut
                print(f"Triggering motion shortcut: {selected_motion_shortcut}")
            else:
                print(f"Skipping duplicate motion shortcut: {selected_motion_shortcut}")
            # await asyncio.sleep(0.3)  # é©å®œã€å¾…æ©Ÿæ™‚é–“ã‚’èª¿æ•´ã—ã¦ãã ã•ã„

        if self.emotion_shortcut_keys:
            # print(f"emotion_shortcut_keys: {self.emotion_shortcut_keys}")
            selected_emotion_shortcut = random.choice(self.emotion_shortcut_keys)
            if selected_emotion_shortcut != self.previous_emotion_shortcut:
                await self.hotkey_trigger.trigger_hotkey(selected_emotion_shortcut)
                self.previous_emotion_shortcut = selected_emotion_shortcut
                print(f"Triggering emotion shortcut: {selected_emotion_shortcut}")
            else:
                print(f"Skipping duplicate emotion shortcut: {selected_emotion_shortcut}")
            # await asyncio.sleep(0.3)  # é©å®œã€å¾…æ©Ÿæ™‚é–“ã‚’èª¿æ•´ã—ã¦ãã ã•ã„


        # èƒŒæ™¯å‹•ç”»ã‚’ã‚¯ãƒªãƒƒãƒ—ã«å¤‰æ›
        background_video = VideoFileClip(self.background_video_path)
        video_duration = background_video.duration

        if hasattr(self, 'background_video_start_time'):
            start_time = self.background_video_start_time
        else:
            start_time = 0

        if round(start_time + audio_duration, 3) > video_duration:
            start_time = 0  # å‹•ç”»ã®é•·ã•ã‚’è¶…ãˆãŸå ´åˆã¯æœ€åˆã‹ã‚‰å†ç”Ÿ

        background_video_clip = background_video.subclip(start_time, start_time + audio_duration)
        self.background_video_start_time = start_time + audio_duration

        # ãƒ›ãƒ¯ã‚¤ãƒˆãƒœãƒ¼ãƒ‰ç”»åƒã‚’ã‚¯ãƒªãƒƒãƒ—ã«å¤‰æ›
        whiteboard_image_clip = ImageClip(frame_data.whiteboard_image_path).set_duration(audio_duration) 

       
        # è§£èª¬ç”»åƒã‚’ã‚¯ãƒªãƒƒãƒ—ã«å¤‰æ›
        # ç”»åƒã‹å‹•ç”»åŒ–ã§åˆ†å²
        # print(f"frame_data.explanation_image_path: {frame_data.explanation_image_path}")
        if frame_data.explanation_image_path.endswith(('.png', '.jpg', '.jpeg','webp')):
            explanation_clip = ImageClip(frame_data.explanation_image_path).set_duration(audio_duration) 
        else:
            # å‹•ç”»ã®é•·ã•ã‚’å°æ•°ç‚¹ç¬¬3ä½ã¾ã§å–å¾—ã—ã€ãã‚ŒãŒä¸€è‡´ã—ã¦ã„ã‚Œã°ç¶šãã‹ã‚‰å†ç”Ÿ
            explanation_video = VideoFileClip(frame_data.explanation_image_path)
            video_duration = round(explanation_video.duration, 3)

            if hasattr(self, 'explanation_video_start_time') and hasattr(self, 'previous_video_duration') and self.previous_video_duration == video_duration:
                start_time = self.explanation_video_start_time
            else:
                start_time = 0

            if round(start_time + audio_duration, 3) > video_duration:
                start_time = 0  # å‹•ç”»ã®é•·ã•ã‚’è¶…ãˆãŸå ´åˆã¯æœ€åˆã‹ã‚‰å†ç”Ÿ

            explanation_clip = explanation_video.subclip(start_time, start_time + audio_duration).set_duration(audio_duration)
            self.explanation_video_start_time = start_time + audio_duration
            self.previous_video_duration = video_duration

        # ã‚¨ãƒ•ã‚§ã‚¯ãƒˆã®è¿½åŠ 
        explanation_clip = self._add_black_frame(explanation_clip)# é»’ã„æ ã®è¿½åŠ 
        explanation_clip = self._fade_clip(explanation_clip, audio_duration,fade_duration=0.3)# è§£èª¬ç”»åƒã®ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ»ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¢ã‚¦ãƒˆ


        # å­—å¹•ç”»åƒã‚’ã‚¯ãƒªãƒƒãƒ—ã«å¤‰æ›
        subtitle_image_clip = ImageClip(frame_data.subtitle_image_path).set_duration(audio_duration)
        # subtitle_image_clip = self._fade_clip(subtitle_image_clip, audio_duration,fade_duration=0.1)# å­—å¹•ç”»åƒã®ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ»ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¢ã‚¦ãƒˆ

        # streaming_thread.join()# ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã®éŒ²ç”»ãŒå®Œäº†ã™ã‚‹ã®ã‚’å¾…ã¤

        # ãƒªã‚µã‚¤ã‚º
        # 1.background_video_clipã‚’ãƒªã‚µã‚¤ã‚º
        background_video_stream = background_video_clip.resize((self.preview_width, self.preview_height))
        background_video_stream = background_video_stream.set_position((0, 0))


        # ãƒ›ãƒ¯ã‚¤ãƒˆãƒœãƒ¼ãƒ‰ç”»åƒã®ä½ç½®ã‚’è¨ˆç®—
        whiteboard_position_w, whiteboard_position_h = 30, 30
        whiteboard_image_clip = whiteboard_image_clip.set_position((whiteboard_position_w, whiteboard_position_h))
        # # é»’å¡—ã‚Šã®ç”»åƒã‚’ä½œæˆ_ãƒ†ã‚¹ãƒˆç”¨
        # black_image = Image.new("RGB", (whiteboard_image_clip.w, whiteboard_image_clip.h), "black") # é»’ã„ç”»åƒã‚’ä½œæˆ
        # black_image_path = save_as_temp_file(black_image)#ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ä½œæˆ
        # whiteboard_bk_image_clip = ImageClip(black_image_path).set_duration(audio_duration) # é»’ã„ç”»åƒã‚’ã‚¯ãƒªãƒƒãƒ—ã«å¤‰æ›
        # whiteboard_bk_image_clip = whiteboard_bk_image_clip.set_position((whiteboard_position_w, whiteboard_position_h))#é»’ã„ç”»åƒã®ä½ç½®ã‚’è¨ˆç®—

        # 5.explanation_clipã‚’ãƒªã‚µã‚¤ã‚º
        # è§£èª¬ç”»åƒã®ã‚µã‚¤ã‚ºã‚’èª¿æ•´
        new_width, new_height = self.resize_aspect_ratio(explanation_clip.w, explanation_clip.h, whiteboard_image_clip.w-10, whiteboard_image_clip.h-10)
        # print(f"new_width: {new_width}")
        # print(f"new_height: {new_height}")
        explanation_clip = explanation_clip.resize((new_width, new_height))
        # è§£èª¬ç”»åƒã®ä½ç½®ã‚’è¨ˆç®—
        explanation_x = (whiteboard_image_clip.w - explanation_clip.w) // 2
        explanation_y = (whiteboard_image_clip.h - explanation_clip.h) // 2
        explanation_clip = explanation_clip.set_position((explanation_x + whiteboard_position_w, explanation_y + whiteboard_position_h))


        # ãƒ›ãƒ¯ã‚¤ãƒˆãƒœãƒ¼ãƒ‰ç”»åƒã‚’åˆæˆ,å­—å¹•ç”»åƒã‚’åˆæˆ
        composed_stream = CompositeVideoClip([
            background_video_stream,
            whiteboard_image_clip,
            # whiteboard_bk_image_clip,
            explanation_clip,
            # streaming_video_clip,
            # subtitle_image_clip,
        ])

        # éŸ³å£°ã‚’è¨­å®š
        composed_stream = composed_stream.set_audio(audio_clip)

        # ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¯ãƒªãƒƒãƒ—ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
        self.frame_clips.append(composed_stream)
        self.subtitle_image_clips.append(subtitle_image_clip)

        # ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰çµæœã‚’å–å¾—
        # audio_thread.join()
        streaming_thread.join()  # ã‚¹ãƒ¬ãƒƒãƒ‰ã®çµ‚äº†ã‚’å¾…ã¤
        # streaming_video_clip = result_queue.get()#ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã®æ˜ åƒã®ã‚¯ãƒªãƒƒãƒ—ã‚’å–å¾—
        # self.streaming_clips.append(streaming_video_clip)



        # self.frame_clips.append(composed_stream.crossfadein(0.5))  # ã‚¯ãƒ­ã‚¹ãƒ•ã‚§ãƒ¼ãƒ‰ã‚’è¿½åŠ 
        # frame_data.frame_clips.append(composed_stream)
        # print(f"frame_data.frame_clips: {composed_stream}")
        # # åº§æ¨™èª¿æ•´ç”¨
        # subtitle_img = Image.open(frame_data.subtitle_image_path).convert("RGBA") # å­—å¹•ç”»åƒã‚’èª­ã¿è¾¼ã‚€       
        # vtuber_img = Image.open(TestData.vtuber_character_path).convert("RGBA")  # Vã‚­ãƒ£ãƒ©ç”»åƒã‚’èª­ã¿è¾¼ã‚€




    # å„éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é †ç•ªã«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã«è¿½åŠ 
    # å¯¾å¿œã™ã‚‹è§£èª¬ç”¨ã®ç”»åƒã‚„å‹•ç”»ã‚’ã€éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®é–‹å§‹ã‹ã‚‰çµ‚äº†ã¾ã§è¡¨ç¤ºã™ã‚‹ã‚ˆã†ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã«è¿½åŠ   
    # ãƒãƒ¼ãƒãƒ£ãƒ«ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§è¡¨ç¤ºã—ãªãŒã‚‰ã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆã™ã‚‹ã€‚ãã—ã¦ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã‚­ãƒ¼ã‚‚ãã®æ™‚ã«å…¥åŠ›ã™ã‚‹ã‚ˆã†ã«ã™ã‚‹ç‚¹
    async def _add_media_to_timeline(self, frame_data: FrameData):

        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†ç”Ÿæ™‚é–“ã‚’å–å¾—ã€€-> ğŸŒŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒãƒ«ãƒ¼ãƒ—å†ç”Ÿã•ã‚Œãªã„ã‚ˆã†ã«ã™ã‚‹ã€‚å†ç”ŸãŒçµ‚ã‚ã£ãŸã‚‰çµ‚äº†ã™ã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
        audio_clip = AudioFileClip(frame_data.audio_file).set_fps(44100)  # 44100ã¯ä¸€èˆ¬çš„ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã§ã™
        audio_duration = audio_clip.duration
        # frame_data.audio_duration = audio_duration


        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã®æ˜ åƒã‚’éŒ²ç”»  
        result_queue = queue.Queue()
        streaming_thread = threading.Thread(target=self._add_streaming_video, args=(audio_duration, frame_data.audio_file))
        streaming_thread.start()
        
        # audio_thread = threading.Thread(target=self.vtuber_play_audio, args=(frame_data.audio_file,))
        # audio_thread.start()


        # ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã‚­ãƒ¼ã®å…¥åŠ›
        # self.hotkeys ã‹ã‚‰ Name ãŒemotion_shortcut_key ã¨ motion_shortcut_key ã¨ä¸€è‡´ã™ã‚‹hotkeyIDã‚’å–å¾—
        self.emotion_shortcut_keys = [hotkey['hotkeyID'] for hotkey in self.hotkeys if hotkey['name'] in frame_data.emotion_shortcut]
        self.motion_shortcut_keys = [hotkey['hotkeyID'] for hotkey in self.hotkeys if hotkey['name'] in frame_data.motion_shortcut]
        
        # ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã‚­ãƒ¼ã®å…¥åŠ›
        if self.motion_shortcut_keys:
            # print(f"motion_shortcut_keys: {self.motion_shortcut_keys}")
            selected_motion_shortcut = random.choice(self.motion_shortcut_keys)
            if selected_motion_shortcut != self.previous_motion_shortcut:
                await self.hotkey_trigger.trigger_hotkey(selected_motion_shortcut)
                self.previous_motion_shortcut = selected_motion_shortcut
                print(f"Triggering motion shortcut: {selected_motion_shortcut}")
            else:
                print(f"Skipping duplicate motion shortcut: {selected_motion_shortcut}")
            # await asyncio.sleep(0.3)  # é©å®œã€å¾…æ©Ÿæ™‚é–“ã‚’èª¿æ•´ã—ã¦ãã ã•ã„

        if self.emotion_shortcut_keys:
            # print(f"emotion_shortcut_keys: {self.emotion_shortcut_keys}")
            selected_emotion_shortcut = random.choice(self.emotion_shortcut_keys)
            if selected_emotion_shortcut != self.previous_emotion_shortcut:
                await self.hotkey_trigger.trigger_hotkey(selected_emotion_shortcut)
                self.previous_emotion_shortcut = selected_emotion_shortcut
                print(f"Triggering emotion shortcut: {selected_emotion_shortcut}")
            else:
                print(f"Skipping duplicate emotion shortcut: {selected_emotion_shortcut}")
            # await asyncio.sleep(0.3)  # é©å®œã€å¾…æ©Ÿæ™‚é–“ã‚’èª¿æ•´ã—ã¦ãã ã•ã„


        # èƒŒæ™¯å‹•ç”»ã‚’ã‚¯ãƒªãƒƒãƒ—ã«å¤‰æ›
        background_video = VideoFileClip(self.background_video_path)
        video_duration = background_video.duration

        if hasattr(self, 'background_video_start_time'):
            start_time = self.background_video_start_time
        else:
            start_time = 0

        if round(start_time + audio_duration, 3) > video_duration:
            start_time = 0  # å‹•ç”»ã®é•·ã•ã‚’è¶…ãˆãŸå ´åˆã¯æœ€åˆã‹ã‚‰å†ç”Ÿ

        background_video_clip = background_video.subclip(start_time, start_time + audio_duration)
        self.background_video_start_time = start_time + audio_duration

        # ãƒ›ãƒ¯ã‚¤ãƒˆãƒœãƒ¼ãƒ‰ç”»åƒã‚’ã‚¯ãƒªãƒƒãƒ—ã«å¤‰æ›
        whiteboard_image_clip = ImageClip(frame_data.whiteboard_image_path).set_duration(audio_duration) 

       
        # è§£èª¬ç”»åƒã‚’ã‚¯ãƒªãƒƒãƒ—ã«å¤‰æ›
        # ç”»åƒã‹å‹•ç”»åŒ–ã§åˆ†å²
        # print(f"frame_data.explanation_image_path: {frame_data.explanation_image_path}")
        if frame_data.explanation_image_path.endswith(('.png', '.jpg', '.jpeg','webp')):
            explanation_clip = ImageClip(frame_data.explanation_image_path).set_duration(audio_duration) 
        else:
            # å‹•ç”»ã®é•·ã•ã‚’å°æ•°ç‚¹ç¬¬3ä½ã¾ã§å–å¾—ã—ã€ãã‚ŒãŒä¸€è‡´ã—ã¦ã„ã‚Œã°ç¶šãã‹ã‚‰å†ç”Ÿ
            explanation_video = VideoFileClip(frame_data.explanation_image_path)
            video_duration = round(explanation_video.duration, 3)

            if hasattr(self, 'explanation_video_start_time') and hasattr(self, 'previous_video_duration') and self.previous_video_duration == video_duration:
                start_time = self.explanation_video_start_time
            else:
                start_time = 0

            if round(start_time + audio_duration, 3) > video_duration:
                start_time = 0  # å‹•ç”»ã®é•·ã•ã‚’è¶…ãˆãŸå ´åˆã¯æœ€åˆã‹ã‚‰å†ç”Ÿ

            explanation_clip = explanation_video.subclip(start_time, start_time + audio_duration).set_duration(audio_duration)
            self.explanation_video_start_time = start_time + audio_duration
            self.previous_video_duration = video_duration

        # ã‚¨ãƒ•ã‚§ã‚¯ãƒˆã®è¿½åŠ 
        explanation_clip = self._add_black_frame(explanation_clip)# é»’ã„æ ã®è¿½åŠ 
        explanation_clip = self._fade_clip(explanation_clip, audio_duration,fade_duration=0.3)# è§£èª¬ç”»åƒã®ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ»ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¢ã‚¦ãƒˆ


        # å­—å¹•ç”»åƒã‚’ã‚¯ãƒªãƒƒãƒ—ã«å¤‰æ›
        subtitle_image_clip = ImageClip(frame_data.subtitle_image_path).set_duration(audio_duration)
        # subtitle_image_clip = self._fade_clip(subtitle_image_clip, audio_duration,fade_duration=0.1)# å­—å¹•ç”»åƒã®ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ»ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¢ã‚¦ãƒˆ

        # streaming_thread.join()# ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã®éŒ²ç”»ãŒå®Œäº†ã™ã‚‹ã®ã‚’å¾…ã¤

        # ãƒªã‚µã‚¤ã‚º
        # 1.background_video_clipã‚’ãƒªã‚µã‚¤ã‚º
        background_video_stream = background_video_clip.resize((self.preview_width, self.preview_height))
        background_video_stream = background_video_stream.set_position((0, 0))


        # ãƒ›ãƒ¯ã‚¤ãƒˆãƒœãƒ¼ãƒ‰ç”»åƒã®ä½ç½®ã‚’è¨ˆç®—
        whiteboard_position_w, whiteboard_position_h = 30, 30
        whiteboard_image_clip = whiteboard_image_clip.set_position((whiteboard_position_w, whiteboard_position_h))
        # # é»’å¡—ã‚Šã®ç”»åƒã‚’ä½œæˆ_ãƒ†ã‚¹ãƒˆç”¨
        # black_image = Image.new("RGB", (whiteboard_image_clip.w, whiteboard_image_clip.h), "black") # é»’ã„ç”»åƒã‚’ä½œæˆ
        # black_image_path = save_as_temp_file(black_image)#ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ä½œæˆ
        # whiteboard_bk_image_clip = ImageClip(black_image_path).set_duration(audio_duration) # é»’ã„ç”»åƒã‚’ã‚¯ãƒªãƒƒãƒ—ã«å¤‰æ›
        # whiteboard_bk_image_clip = whiteboard_bk_image_clip.set_position((whiteboard_position_w, whiteboard_position_h))#é»’ã„ç”»åƒã®ä½ç½®ã‚’è¨ˆç®—

        # 5.explanation_clipã‚’ãƒªã‚µã‚¤ã‚º
        # è§£èª¬ç”»åƒã®ã‚µã‚¤ã‚ºã‚’èª¿æ•´
        new_width, new_height = self.resize_aspect_ratio(explanation_clip.w, explanation_clip.h, whiteboard_image_clip.w-10, whiteboard_image_clip.h-10)
        # print(f"new_width: {new_width}")
        # print(f"new_height: {new_height}")
        explanation_clip = explanation_clip.resize((new_width, new_height))
        # è§£èª¬ç”»åƒã®ä½ç½®ã‚’è¨ˆç®—
        explanation_x = (whiteboard_image_clip.w - explanation_clip.w) // 2
        explanation_y = (whiteboard_image_clip.h - explanation_clip.h) // 2
        explanation_clip = explanation_clip.set_position((explanation_x + whiteboard_position_w, explanation_y + whiteboard_position_h))


        # ãƒ›ãƒ¯ã‚¤ãƒˆãƒœãƒ¼ãƒ‰ç”»åƒã‚’åˆæˆ,å­—å¹•ç”»åƒã‚’åˆæˆ
        composed_stream = CompositeVideoClip([
            background_video_stream,
            whiteboard_image_clip,
            # whiteboard_bk_image_clip,
            explanation_clip,
            # streaming_video_clip,
            # subtitle_image_clip,
        ])

        # éŸ³å£°ã‚’è¨­å®š
        composed_stream = composed_stream.set_audio(audio_clip)

        # ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¯ãƒªãƒƒãƒ—ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
        self.frame_clips.append(composed_stream)
        self.subtitle_image_clips.append(subtitle_image_clip)

        # ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰çµæœã‚’å–å¾—
        # audio_thread.join()
        streaming_thread.join()  # ã‚¹ãƒ¬ãƒƒãƒ‰ã®çµ‚äº†ã‚’å¾…ã¤
        # streaming_video_clip = result_queue.get()#ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã®æ˜ åƒã®ã‚¯ãƒªãƒƒãƒ—ã‚’å–å¾—
        # self.streaming_clips.append(streaming_video_clip)



        # self.frame_clips.append(composed_stream.crossfadein(0.5))  # ã‚¯ãƒ­ã‚¹ãƒ•ã‚§ãƒ¼ãƒ‰ã‚’è¿½åŠ 
        # frame_data.frame_clips.append(composed_stream)
        # print(f"frame_data.frame_clips: {composed_stream}")
        # # åº§æ¨™èª¿æ•´ç”¨
        # subtitle_img = Image.open(frame_data.subtitle_image_path).convert("RGBA") # å­—å¹•ç”»åƒã‚’èª­ã¿è¾¼ã‚€       
        # vtuber_img = Image.open(TestData.vtuber_character_path).convert("RGBA")  # Vã‚­ãƒ£ãƒ©ç”»åƒã‚’èª­ã¿è¾¼ã‚€




